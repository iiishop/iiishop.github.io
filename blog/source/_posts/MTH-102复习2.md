---
title: MTH-102复习2
date: 2023-05-13 21:42:34
tags:
- XJTLU
- 笔记
- 概率论
categories:
- 中文推文
- 学习
- 概率论
---

随机变量上半部分，包括基础内容和离散随机变量

<!--more-->

# Lec5
1. ## Random variables
	1. ### Definition
		Given a random experiment with a sample space *S*, a function *X* that assigns one and only one real number *X(s) = x* to each element *s* in *S* is called a **random variable**. The **space** of *X* is the set of real numbers$$\{x:X(s)=x,s\in S\}$$
		where $s\in S$ means that the element *s* belongs to the set *S*
		随机变量*X*将*S*样本空间中的抽象的概念用数字表现出来。
	2. ### Probability mass function (pmf)
		Let *X* be a discrete random variable and $x_{1},x_{2},...$ be the values that *X* can take on. The pmf p(x) is a function that satisfies the following properties:
			1. $p(x_{i})\geq0,\ i=1,2,...$
			2. $\sum\limits_{i=1}^{\infty}p(x_{i})=1$
			3. $P(X\in A)=\sum\limits_{x_{i}\in A}p(x_{i})$, for any event A.
		pmf指概率质量函数。在概率论中，概率质量函数是离散随机变量在各特定取值上的概率
		例如，投掷一枚硬币，令：$$X=\left\{\begin{array}{lc}1&硬币投掷结果为正面\\0&硬币投掷结果为反面\\\end{array}\right.$$那么，其PMF为$$p(x_{i})=\left\{\begin{array}{lc}\frac{1}{2}&i=0\\\frac{1}{2}&i=1\\0&i\notin\set{0,1}\end{array}\right.$$
	3. ### Cumulative distribution function (cdf)
		- We call the function defined by$$F(x)=P(X\leq x),-\infty<x<\infty$$the **cumulative distribution function** and abbreviate it as **cdf**
		- Let $p(x)$ be the pmf of the random variable $X$, and $x_{1},x_{2},...$ be the values that $X$ can take on. Then for any $x\in\mathbb{R}$$$F(x)=\sum\limits_{x_{i}\leq x}p(x_{i})$$
		- For any $x\in \mathbb{R}$,$$0\leq F(x)\leq1,\ \lim_{x\rightarrow-\infty}F(x)=0,\ \lim_{x\rightarrow\infty}F(x=1)$$
		- $F(·)$ is a nondecreasing function, i.e. $F(x)\leq F(y)$ for $x\leq y$.
		- 同样是上面的硬币问题，其cdf为$$F(x)=P(X\leq x)=\left\{\begin{array}{lc}0&x<0\\\frac{1}{2}&0\leq x<1\\1&x\geq 1\end{array}\right.$$
		- #### Properties of cdf
			1. The cdf $F(·)$ is right continuous, i.e. for any $x\in \mathbb{R}$$$F(x)=F(x+0)$$where $F(x+0)$ is the right limit of $F$ at $x$
			2. For any $x\in \mathbb{R}$,$$P(X=x)=F(x)-F(x-0)$$$$P(X<x)=P(X\leq x)-P(X=x)=F(x-0)$$where $F(x-0)$ is the left limit of $F$ at $x$
			3. For $a<b$,$$P(a<X\leq b)=P(X\leq b)-P(X\leq a)=F(b)-F(a)$$$$P(a\leq X<b)=P(X<b)-P(X<a)=F(b-0)-F(a-0)$$
1. ## Mean and variance
	1. ### Definition of mean
		If $X$ is a discrete random variable having a probability mass function $p(x)$, then the mean, or the expectation, of $X$, denoted by $E(X)$, is defined by$$E[X]=\sum\limits_{x:p(x)>0}xp(x)$$iIf the values that $X$ can take on are $x_{1},x_{2},...$, then$$E[X]=\sum\limits_{i=1}^{\infty}x_{i}p(x_{i})$$In words, the mean of $X$ is a weighted average of the possible values that $X$ can take on, each value being weighted by the probability that $X$ assumes it.
	2. ### Expectation of a function of a random variable
		If *X* is a discrete random variable that takes on the values $x_{1},x_{2},...$, with the pmf *p(x)*, then for any real-valued function $g:\mathbb{R}\rightarrow\mathbb{R}$$$E[g(X)]=\sum\limits_{i=1}^{\infty}g(x_{i})p(x_{i})$$**Remark** $Y=g(X)$ is a actually a new random variable. However, to compute the mean of $Y$, i.e. $E[Y]=E[g(X)]$, it is not necessary to find the distribution of $Y$ once the distribution of $X$ is given.
	3. ### Definition of variance
		The variance of a random variable $X$, denoted by $Var(X)$, is defined as the mean of the function of $X$ with $g(X)=(X-E[X])^{2}$, i.e. $$Var(X)=E[(X-E[X])^{2}]$$In the practice, it is more convenient to compute the variance vir the following equivalent formula $$Var(X)=E[X^{2}]-(E[X])^{2}$$
		- The variance is always nonnegative
		- The square root of $Var(X)$, i.e. $\sqrt{Var(X)}$, is called the standard deviation of $X$
	4. ### Properties of mean and variance
		Let $X$ be a random variable, $a, b$, and $c$ be constants. Consider $Y=aX+b$ as a linear function of $X$. Then
		- $$E(c)=c,Var(c)=0$$
		- $$E(Y)=E(aX+b)=aE(X)+b$$
		- $$Var(Y)=Var(aX+b)=a^{2}Var(X)$$
	
		The formula $Var(X)=E[X^{2}]-(E[X])^{2}$ can be proved using the above properties
		- Let $X_{1},X_{2},...,X_{n}$ be random variables. The mean of a sum of random variables equals the sum of the mean of each random variable, i.e.$$E(\sum\limits_{i=1}^{n}X_{i})=\sum\limits_{i=1}^{n}E(X_{i})$$Moreover, let $a_{1},a_{2},...,a_{n}$ be constant. Then$$E(\sum\limits_{i=1}^{n}a_{i}X_{i})=\sum\limits_{i=1}^{n}a_{i}E(X_{i})$$

# Lec6
离散随机变量(Discrete random)指的是一些值取得不连续的随机变量，上述所有的定义和概念都建立在随机变量是离散随机变量的情况下。
下面会介绍一些简单的离散随机变量

1. ## Bernoulli distribution
	- A **Bernoulli experiment** is a random experiment with two outcomes, modeled with the sample space $$S=\{0,1\}$$
	- Let $X$ be a random variable associated with a Bernoulli experiment with $$P(X=1)=p,\ P(X=0)=1-p$$for some $0\leq p\leq 1$. The pmf of $X$ can be written as $$p(x)=p^{x}(1-p)^{1-x},\ x=0,1$$
	- We say that $X$ has a **Bernoulli distribution**, and $$\mu=E[X] =(0)(1-p)+(1)(p)=p$$$$\sigma^{2}=Var(X)=(0-p)^{2}(1-p)+(1-p)^{2}p=p(1-p)$$
1. ## Binomial distribution
	- A Bernoulli experiment is performed $n$ times independently, and let the random variable $X$ be the number of times when the outcome is 1 in the $n$ trials
	- The sample space of $X$ is $S=\{0,1,...,n\}$
	- The pmf of $X$ is $$p(k)=P(X=k)={n\choose k}p^{k}(1-p)^{n-k},\ k=0,1,...,n$$
	- X is said to have a **binomial distribution**, which is denoted by the symbol $b(n,p)$. The constants $n$ and $p$ are called the **parameters** of the binomial distribution
	- A Bernoulli distribution is just a binomial distribution with parameters $(1,p)$
	- If a random variable $X$ has a binomial distribution with parameters $(n,p)$, then $$E(X)=np,\ Var(X)=np(1-p)$$The mean and variance can be computed in two ways
		1. Direct computation with series:$$E(X)=\sum\limits_{k=0}^{n}k{n\choose k}p^{k}(1-p)^{n-k}=np\sum\limits_{k=1}^{n-1}{n-1\choose k-1}p^{k-1}(1-p)^{k-1}=np$$$$Var(X)=E(X^{2})-[E(X)]^{2}=\sum\limits_{k=0}^{n}k^{2}{n\choose k}p^{k}(1-p)^{n-k}-(np)^{2}=np(1-p)$$
		2. For $i=1,2,...,n$, let $X_{i}$ be the outcome of the i-th trial. Then $X_{i}$ has a Bernoulli distribution ($E(X_{i})=p,\ Var(X_{i})=p(1-p)$) and $$X=X_{1}+X_{2}+...+X_{n}$$Therefore,$$E(X)=E(X_{1})+...+E(X_{n})=np$$$$Var(X)=Var(X_{1})+...+Var(X_{n})=np(1-p)$$
1. ## Geometric distribution
	- Motivation: we are interested in the number of independent trials needed in order to get the first success. The probability of success in each trial is constantly $p\in (0,1)$
	- The sample space $S=\{1,2,3,...\}$
	- A random variable $X$ is said to have a **geometric distribution** if the pmf of $X$ is defined by$$p(k)=P(X=k)=q^{k-1}p,\ k=1,2,3,...,$$where $0<p<1,\ q=1-p$
	- The cdf of a geometric random variable $X$ is $$P(X\leq k)=\sum\limits_{i=1}^{k}q^{i-1}p=1-q^{k},\ k=1,2,...$$
	- The mean of $X$ is $$E(X)=\frac{1}{p}$$
	- The variance of $X$ is $$Var(X)=\frac{1-p}{p^{2}}$$
1. ## Poisson distribution
	- The pmf$$p(k)=P(X=k)=\frac{\lambda^{k}}{k!}e^{-\lambda},\ k=0,1,2,...$$where $\lambda>0$
	- The mean of $X$ is$$E(X)=\lambda$$
	- The variance of $X$ is $$Var(X)=\lambda$$
	- $\lambda$通常为数据的均值
	- 泊松分布是由二项分布推导得来。但二项分布的p很小时，两者比较接近